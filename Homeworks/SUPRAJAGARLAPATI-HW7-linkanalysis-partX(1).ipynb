{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 To Create a crawler using the webcrawler provided, have it crawl the first 200 pages from a base URL of your choosing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are the steps to create the crawler, that I followed from my lecture https://github.com/pschragger/big-data-python-class/blob/master/Lectures/Lecture_7_-_Link_Analysis.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the directory on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 6.3.9600]\r\n",
      "(c) 2013 Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads>mkdir Homework7\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads>"
     ]
    }
   ],
   "source": [
    "%%cmd C:\\Users\\SUPRAJA GARLAPATI\\big-data-python-class\\Homeworks\n",
    "mkdir Homework7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching to the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\n"
     ]
    }
   ],
   "source": [
    "cd Homework7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 6.3.9600]\r\n",
      "(c) 2013 Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7>scrapy startproject iot\n",
      "New Scrapy project 'iot', using template directory 'C:\\\\Users\\\\SUPRAJA GARLAPATI\\\\Anaconda2\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project', created in:\r\n",
      "    C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd iot\r\n",
      "    scrapy genspider example example.com\r\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "scrapy startproject iot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot\n"
     ]
    }
   ],
   "source": [
    "cd iot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the spider named iotbykarthik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 6.3.9600]\r\n",
      "(c) 2013 Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot>scrapy genspider iotbykarthik https://iotbykarthik.wordpress.com\n",
      "Created spider 'iotbykarthik' using template 'basic' in module:\r\n",
      "  iot.spiders.iotbykarthik\r\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "scrapy genspider iotbykarthik https://iotbykarthik.wordpress.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot\\iot\n"
     ]
    }
   ],
   "source": [
    "cd iot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot\\iot\\spiders\n"
     ]
    }
   ],
   "source": [
    "cd spiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iotbykarthik.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iotbykarthik.py\n",
    "import scrapy\n",
    "\n",
    "class iotkarthik(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    resp = scrapy.Field()\n",
    "    \n",
    "class iotbykarthikSpider(scrapy.Spider):\n",
    "    name = 'iotbykarthik'\n",
    "    allowed_domains = ['https://iotbykarthik.wordpress.com']\n",
    "    start_urls = ['https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        hxs = scrapy.Selector(response)\n",
    "        titles = hxs.xpath('//ul/li')\n",
    "        item = []\n",
    "        for title in titles:\n",
    "            obj = iotkarthik()\n",
    "            obj[\"title\"] = title.xpath(\"a/text()\").extract()\n",
    "            obj[\"link\"] = title.xpath(\"a/@href\").extract()\n",
    "            obj[\"resp\"] = response\n",
    "            if obj[\"title\"] != []:\n",
    "                item.append(obj)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Crawled urls are saved into a csv file with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 6.3.9600]\r\n",
      "(c) 2013 Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot\\iot\\spiders>scrapy crawl iotbykarthik -o iotbykarthik_txt.csv -t csv\n",
      "\r\n",
      "(C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2) C:\\Users\\SUPRAJA GARLAPATI\\Downloads\\Homework7\\iot\\iot\\spiders>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-02 17:57:23 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: iot)\r\n",
      "2017-12-02 17:57:23 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'iot.spiders', 'FEED_URI': 'iotbykarthik_txt.csv', 'SPIDER_MODULES': ['iot.spiders'], 'BOT_NAME': 'iot', 'ROBOTSTXT_OBEY': True, 'FEED_FORMAT': 'csv'}\r\n",
      "2017-12-02 17:57:28 [scrapy.middleware] INFO: Enabled extensions:\r\n",
      "['scrapy.extensions.feedexport.FeedExporter',\r\n",
      " 'scrapy.extensions.logstats.LogStats',\r\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\r\n",
      " 'scrapy.extensions.corestats.CoreStats']\r\n",
      "2017-12-02 17:57:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n",
      "2017-12-02 17:57:33 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n",
      "2017-12-02 17:57:33 [scrapy.middleware] INFO: Enabled item pipelines:\r\n",
      "[]\r\n",
      "2017-12-02 17:57:33 [scrapy.core.engine] INFO: Spider opened\r\n",
      "2017-12-02 17:57:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n",
      "2017-12-02 17:57:34 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\r\n",
      "2017-12-02 17:57:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://iotbykarthik.wordpress.com/robots.txt> (referer: None)\r\n",
      "2017-12-02 17:57:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/> (referer: None)\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'#comment-form-guest'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'\\n\\t\\t\\t\\t\\t', u'\\n\\t\\t\\t\\t']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'#comment-form-load-service:WordPress.com'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'\\n\\t\\t\\t\\t\\t', u'\\n\\t\\t\\t\\t']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'#comment-form-load-service:Twitter'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'\\n\\t\\t\\t\\t\\t', u'\\n\\t\\t\\t\\t']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'#comment-form-load-service:Facebook'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'\\n\\t\\t\\t\\t\\t', u'\\n\\t\\t\\t\\t']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/05/02/basket-billing-specs/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Basket Billing (Specs)']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Raspberry PI, Node Red and IBM\\xa0Watson']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/03/16/arduino-node-red-and-ui-to-switch-led/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Arduino Node red and UI to switch\\xa0LED']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/02/16/pi-as-wifi-access-pointap/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'PI as WIFI Access\\xa0Point(AP)']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/02/08/tutorial-on-arduino-and-mqtt/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Tutorial on Arduino and\\xa0MQTT']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/05/02/basket-billing-specs/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Basket Billing (Specs)']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Raspberry PI, Node Red and IBM\\xa0Watson']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/03/16/arduino-node-red-and-ui-to-switch-led/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Arduino Node red and UI to switch\\xa0LED']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/02/16/pi-as-wifi-access-pointap/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'PI as WIFI Access\\xa0Point(AP)']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/2016/02/08/tutorial-on-arduino-and-mqtt/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Tutorial on Arduino and\\xa0MQTT']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://wordpress.com/start?ref=wplogin'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Register']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/wp-login.php'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Log in']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/feed/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Entries ']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://iotbykarthik.wordpress.com/comments/feed/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'Comments ']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>\r",
      "\r\n",
      "{'link': [u'https://wordpress.com/'],\r\n",
      " 'resp': <200 https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/>,\r\n",
      " 'title': [u'WordPress.com']}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.engine] INFO: Closing spider (finished)\r\n",
      "2017-12-02 17:57:37 [scrapy.extensions.feedexport] INFO: Stored csv feed (19 items) in: iotbykarthik_txt.csv\r\n",
      "2017-12-02 17:57:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n",
      "{'downloader/request_bytes': 506,\r\n",
      " 'downloader/request_count': 2,\r\n",
      " 'downloader/request_method_count/GET': 2,\r\n",
      " 'downloader/response_bytes': 17806,\r\n",
      " 'downloader/response_count': 2,\r\n",
      " 'downloader/response_status_count/200': 2,\r\n",
      " 'finish_reason': 'finished',\r\n",
      " 'finish_time': datetime.datetime(2017, 12, 2, 22, 57, 37, 478000),\r\n",
      " 'item_scraped_count': 19,\r\n",
      " 'log_count/DEBUG': 22,\r\n",
      " 'log_count/INFO': 8,\r\n",
      " 'response_received_count': 2,\r\n",
      " 'scheduler/dequeued': 1,\r\n",
      " 'scheduler/dequeued/memory': 1,\r\n",
      " 'scheduler/enqueued': 1,\r\n",
      " 'scheduler/enqueued/memory': 1,\r\n",
      " 'start_time': datetime.datetime(2017, 12, 2, 22, 57, 34, 589000)}\r\n",
      "2017-12-02 17:57:37 [scrapy.core.engine] INFO: Spider closed (finished)\r\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "scrapy crawl iotbykarthik -o iotbykarthik_txt.csv -t csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 To create a Stochastic matrix from its resulting crawling as per https://cs7083.wordpress.com/2013/01/31/demystifying-the-pagerank-and-hits-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def pagerank(H):\n",
    "    n = len(H)\n",
    "    w = zeros(n)\n",
    "    rho = 1./n * ones(n);\n",
    "    for i in range(n):\n",
    "        if multiply.reduce(H[i]== zeros(n)):\n",
    "            w[i] = 1\n",
    "    newH = H + outer((1./n * w),ones(n))\n",
    " \n",
    "    theta=0.85\n",
    "    G = (theta * newH) + ((1-theta) * outer(1./n * ones(n), ones(n)))\n",
    "    #print rho\n",
    "    for j in range(10):\n",
    "        rho = dot(rho,G)\n",
    "        #print rho\n",
    "    \n",
    "    return list(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUPRAJA GARLAPATI\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. ,  0. ,  0.5,\n",
       "         0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         1. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  1. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  1. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  1. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "lst = []\n",
    "raw = pd.read_csv(\"iotbykarthik_txt.csv\")\n",
    "raw['link'] = 'https://iotbykarthik.wordpress.com'+raw['link']\n",
    "r = raw['resp'][0]\n",
    "lst.append((r.split()[-1]).split('>')[-2])\n",
    "for i in range(len(raw)):\n",
    "    if raw['resp'][i] == r:\n",
    "        lst.append(raw['link'][i])\n",
    "    else:\n",
    "        r = raw['resp'][i]\n",
    "        lst.append((r.split()[-1]).split('>')[-2])\n",
    "        \n",
    "item = list(pd.DataFrame(lst)[0].unique())\n",
    "\n",
    "link = []\n",
    "length = len(lst)\n",
    "for i, val in enumerate(lst):\n",
    "    if i < length-1:\n",
    "        link.append((lst[i], lst[i+1]))\n",
    "\n",
    "n = pd.DataFrame(index=item, columns=item)\n",
    "m = n.replace(np.NaN, 0)\n",
    "for i in link:\n",
    "    m.loc[i] = 1.0\n",
    "\n",
    "ar = np.array(m)\n",
    "v = ar.sum(axis=1)\n",
    "\n",
    "result = ar/v[:, np.newaxis]\n",
    "matrix = np.nan_to_num(result)\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 Pass it through the Page Rank algorithm and provide the list of the top 5 page URLs in our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please create matrix again after interpreting pagerank algorithm\n",
    "k = pagerank(matrix)\n",
    "value = ((pd.DataFrame(k)).sort_values(0, ascending=False)).head(5)\n",
    "index = list(value.index)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five URLs:\n",
      "https://iotbykarthik.wordpress.comhttps://iotbykarthik.wordpress.com/2016/05/02/basket-billing-specs/\n",
      "https://iotbykarthik.wordpress.comhttps://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/\n",
      "https://iotbykarthik.wordpress.comhttps://iotbykarthik.wordpress.com/2016/03/16/arduino-node-red-and-ui-to-switch-led/\n",
      "https://iotbykarthik.wordpress.comhttps://iotbykarthik.wordpress.com/2016/02/16/pi-as-wifi-access-pointap/\n",
      "https://iotbykarthik.wordpress.comhttps://iotbykarthik.wordpress.com/2016/02/08/tutorial-on-arduino-and-mqtt/\n"
     ]
    }
   ],
   "source": [
    "url = pd.DataFrame(item)\n",
    "print \"Top five URLs:\"\n",
    "for i in index:\n",
    "    print url[0][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4 Using the hits algorithm ( with a connectivity matrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hits(A):\n",
    "    n= len(A)\n",
    "    Au= dot(transpose(A),A)\n",
    "    Hu = dot(A,transpose(A))\n",
    "    a = ones(n); h = ones(n)\n",
    "    #print a,h\n",
    "    for j in range(5):\n",
    "        a = dot(a,Au)\n",
    "        a= a/sum(a)\n",
    "        h = dot(h,Hu)\n",
    "        h = h/ sum(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value = pd.DataFrame(hits(ar))\n",
    "sort = (value.sort_values(0, ascending=False)).head(5)\n",
    "hit = list(sort.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 page URLs:\n",
      "https://iotbykarthik.wordpress.com/2016/02/08/tutorial-on-arduino-and-mqtt/\n",
      "https://iotbykarthik.wordpress.com#comment-form-load-service:Facebook\n",
      "https://iotbykarthik.wordpress.com/2016/04/11/raspberry-pi-node-red-and-ibm-watson/\n",
      "https://iotbykarthik.wordpress.com#comment-form-guest\n",
      "https://iotbykarthik.wordpress.com\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = pd.DataFrame(item)\n",
    "print \"Top 5 page URLs:\"\n",
    "for i in hit:\n",
    "    print 'https://iotbykarthik.wordpress.com'+(url[0][i]).split('.com')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Algorithms :\n",
    "    Ranking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising. Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries.\n",
    "    I tried to implement the other ranking algorithms but could not find a training data part to implement those. I searched for reditt and SVM algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://iotbykarthik.wordpress.com\n",
    "http://localhost:8888/notebooks/Downloads/big-data-python-class-master/Lectures/Lecture_7_-_Link_Analysis.ipynb\n",
    "https://en.wikipedia.org/wiki/PageRank                                \n",
    "https://www.slideshare.net/Ankit007_/ranking-algorithms                         \n",
    "http://www.totallycommunications.com/latest/search-engine-basics-crawling-indexing-ranking/                                           \n",
    "https://cs7083.wordpress.com/2013/01/31/demystifying-the-pagerank-and-hits-algorithms/                                                                                                                      \n",
    "https://doc.scrapy.org/en/latest/intro/tutorial.html                                                                  \n",
    "https://doc.scrapy.org/en/latest/intro/install.html#intro-install  \n",
    "https://en.wikipedia.org/wiki/Learning_to_rank\n",
    "https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
